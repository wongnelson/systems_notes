## What is a Double Fault?

In simplified terms, a double fault is a special type of exception that occurs when the CPU fails to invoke an exception handler. For instance, it occurs when a page fault is triggered but there's no page fault handler registered in the Interrupt Descriptor Table (IDT). It's akin to catch-all blocks in programming languages that support exceptions, such as `catch(...)` in C++ or `catch(Exception e)` in Java or C#.

A double fault behaves like a normal exception. It has the vector number 8 and we can define a normal handler function for it in the IDT. It's vital to provide a double fault handler because if a double fault is unhandled, a fatal triple fault happens. Triple faults can't be caught, and most hardware responds with a system reset.

## How can we trigger a Double Fault?

We can provoke a double fault by triggering an exception for which we didn't define a handler function:

```rust
// in src/main.rs

#[no_mangle]
pub extern "C" fn _start() -> ! {
    println!("Hello World{}", "!");

    blog_os::init();

    // trigger a page fault
    unsafe {
        *(0xdeadbeef as *mut u8) = 42;
    };

    // as before
    #[cfg(test)]
    test_main();

    println!("It did not crash!");
    loop {}
}
```

Here, we're using `unsafe` to write to the invalid address `0xdeadbeef`. The virtual address isn't mapped to a physical address in the page tables, so a page fault happens. As we haven’t registered a page fault handler in our IDT, a double fault occurs.

When we start our kernel now, we find it entering an endless boot loop. This happens because of the following:

1. The CPU attempts to write to `0xdeadbeef`, causing a page fault.
2. The CPU looks at the corresponding entry in the IDT and observes that no handler function is specified. Therefore, it can’t call the page fault handler and a double fault occurs.
3. The CPU then looks at the IDT entry of the double fault handler, but this entry also does not specify a handler function. Consequently, a triple fault occurs.
4. A triple fault is fatal. QEMU reacts to it like most real hardware and issues a system reset.

To avoid this triple fault, we either need to provide a handler function for page faults or a double fault handler. Our goal is to evade triple faults in all situations, so let's start with a double fault handler that is invoked for all unhandled exception types.

## How can we handle Double Faults?

A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:

```rust
// in src/interrupts.rs

lazy_static! {
    static ref IDT: InterruptDescriptorTable = {
        let mut idt = InterruptDescriptorTable::new();
        idt.breakpoint.set_handler_fn(breakpoint_handler);
        idt.double_fault.set_handler_fn(double_fault_handler); // new
        idt
    };
}

// new
extern "x86-interrupt" fn double_fault_handler(
    stack_frame: InterruptStackFrame, _error_code: u64) -> !
{
    panic!("EXCEPTION: DOUBLE FAULT\n{:#?}", stack_frame);
}
```

Here, our handler prints a brief error message and dumps the exception stack frame. The error code of the double fault handler is always zero, so there’s no reason to print it. A key difference to the breakpoint handler is that the double fault handler is diverging, meaning it doesn't return. The reason for this is that the x86_64 architecture doesn't allow returning from a double fault exception.

When we now start our kernel, we should see that the double fault handler is invoked:

```
QEMU printing EXCEPTION: DOUBLE FAULT and the exception stack frame
```

Here's what happens this time:

1. The CPU attempts to write to `0xdeadbeef`, causing a page fault.
2. Like before, the CPU looks at the corresponding entry in the IDT and sees that no handler function is defined. Thus, a double fault occurs.
3. The CPU jumps to the – now present – double fault handler.
4. The triple fault (and the boot-loop) no longer occurs, since the CPU can now call the double fault handler.

This setup allows us to catch most double faults, but there are some cases where our current approach might be insufficient.

## What Exactly Causes Double Faults?

To understand the specific cases of double faults, we need to delve into what precisely causes them. We previously defined a double fault in somewhat vague terms: a special exception that happens when the CPU fails to invoke an exception handler.

But what exactly does "fails to invoke" mean? Could it be that the handler isn't present, or perhaps it's swapped out? And what happens if an exception is caused by a handler itself?

Let's consider some scenarios:

- What if a breakpoint exception occurs, but the corresponding handler function is swapped out?
- What if a page fault occurs, but the page fault handler is swapped out?
- What if a divide-by-zero handler triggers a breakpoint exception, but the breakpoint handler is swapped out?
- What if our kernel overflows its stack and hits the guard page?

Fortunately, the AMD64 manual provides a precise definition (in Section 8.2.9): "a double fault exception can occur when a second exception happens during the handling of a prior (first) exception handler". Note the term "can": only certain combinations of exceptions lead to a double fault. Here are those combinations:

| First Exception | Second Exception |
| --- | --- |
| Divide-by-zero, Invalid TSS, Segment Not Present, Stack-Segment Fault, General Protection Fault | Invalid TSS, Segment Not Present, Stack-Segment Fault, General Protection Fault |
| Page Fault | Page Fault, Invalid TSS, Segment Not Present, Stack-Segment Fault, General Protection Fault |

So, for instance, a divide-by-zero fault followed by a page fault is fine—the page fault handler is invoked. But a divide-by-zero fault followed by a general-protection fault results in a double fault.

With this table, we can answer the earlier questions:

1. If a breakpoint exception occurs and the corresponding handler function is swapped out, a page fault happens, and the page fault handler is invoked.
2. If a page fault happens and the page fault handler is swapped out, a double fault happens, and the double fault handler is invoked.
3. If a divide-by-zero handler causes a breakpoint exception, the CPU attempts to invoke the breakpoint handler. If the breakpoint handler is swapped out, a page fault happens, and the page fault handler is invoked.

In fact, even the case of an exception without a handler function in the IDT follows this scheme: When the exception occurs, the CPU tries to read the corresponding IDT entry. As the entry is 0, which isn't a valid IDT entry, a general protection fault occurs. We didn't define a handler function for the general protection fault either, so another general protection fault happens. According to the table, this leads to a double fault.

## What Happens when the Kernel Stack Overflows?

The kernel stack overflowing is an interesting situation. We can depict this by the scenario where the guard page is hit. Here's what happens in this case.

### What is a Guard Page?

A guard page is a unique memory page at the bottom of a stack that allows us to detect stack overflows. This page is not mapped to any physical frame, so any attempt to access it will result in a page fault, not silent corruption of other memory. 

```ascii
        +-------------+
        |             |
        |  Stack      |
        |             |
        +-------------+
        | Guard Page  | <----- Stack Overflow Hits Here
        +-------------+
        |             |
        |             |
```

### What Happens When Guard Page is Hit?

When the guard page is accessed, a page fault occurs. The CPU will then look up the page fault handler in the IDT (Interrupt Descriptor Table) and try to push the interrupt stack frame onto the stack. However, since the current stack pointer still points to the non-present guard page, a second page fault happens, resulting in a double fault.

### What Does the CPU do After a Double Fault?

Now, when the page fault exception is triggered due to an access to the guard page, the CPU attempts to handle this exception. But to do that, it needs to push an exception stack frame onto the stack. The problem is that the stack pointer is still pointing at the guard page (because that's where it was trying to write when the initial page fault occurred). As a result, this attempt to push the exception stack frame also accesses the guard page, causing another page fault - this is the double fault.

Even in the double fault handler, the CPU still needs to push an exception stack frame, which, again, will attempt to write to the guard page because the stack pointer hasn't moved. Since the stack pointer still points to the guard page, a third page fault occurs, leading to a triple fault and subsequently, a system reboot. 

## How to Provoke a Kernel Stack Overflow?

We can provoke a kernel stack overflow by calling a function that recurses endlessly. Here is an example using Rust:

```rust
// in src/main.rs

#[no_mangle] // don't mangle the name of this function
pub extern "C" fn _start() -> ! {
    println!("Hello World{}", "!");

    blog_os::init();

    fn stack_overflow() {
        stack_overflow(); // for each recursion, the return address is pushed
    }

    // trigger a stack overflow
    stack_overflow();

    […] // test_main(), println(…), and loop {}
}
```

When we run this code in QEMU, we'll notice that the system enters a bootloop again. 

## How to Avoid Kernel Stack Overflow?

To prevent the kernel stack overflow, we need to ensure that the stack is always valid when a double fault exception happens. The x86_64 architecture provides a solution for this: switching stacks at the hardware level before the CPU pushes the exception stack frame.

This switch is implemented through an Interrupt Stack Table (IST), a table of seven pointers to known-good stacks.

```rust
struct InterruptStackTable {
    stack_pointers: [Option<StackPointer>; 7],
}
```

We can assign each exception handler a stack from the IST through the stack_pointers field in the corresponding IDT entry. If our double fault handler uses the first stack in the IST, the CPU will automatically switch to this stack whenever a double fault happens, preventing the triple fault.

## What is the Relationship between IST and TSS?

The Interrupt Stack Table (IST) is part of an old legacy structure called Task State Segment (TSS). The TSS used to hold various pieces of information about a task in 32-bit mode. In 64-bit mode, however, hardware context switching is no longer supported and the format of the TSS has changed completely.

In x86_64, the TSS no longer holds any task-specific information. Instead, it holds two stack tables—the IST is one of them. The only common field between the 32-bit and 64-bit TSS is the pointer to the I/O port permissions bitmap.

The 64-bit TSS has the following format:

```ascii
Field                     Type
(reserved)                u32
Privilege Stack Table     [u64; 3]
(reserved)                u64
Interrupt Stack Table     [u64; 7]
(reserved)                u64
(reserved)                u16
I/O Map Base Address      u16
```

The Privilege Stack Table is used by the CPU when the privilege level changes. For example, if an exception occurs while the CPU is in user mode (privilege level 3), the CPU normally switches to kernel mode (privilege level 0) before invoking the exception handler. In that case, the CPU would switch to the 0th stack in the Privilege Stack Table (since 0 is the target privilege level). We don’t have any user-mode programs yet, so we will ignore this table for now.

### What's the TSS?
The Task State Segment (TSS) is a special data structure in x86 architecture, which holds information about a task. The TSS is important for hardware-based task-switching, where the CPU automatically saves and restores certain state, and for switching stacks when transitioning between different privilege levels or handling interrupts and exceptions.

However, in the x86_64 (also known as 64-bit or long mode) architecture, most of the TSS fields are obsolete, as hardware task-switching is no longer supported. The main use of the TSS in 64-bit mode is for holding the stack pointers (RSP0, RSP1, RSP2) used when transitioning to a different privilege level and the pointer to the I/O Permission Bitmap.

A simplified structure of the TSS in x86_64 mode might look something like this:

```
+------------------------+
|   (Unused)             |
+------------------------+
|   Reserved             |
+------------------------+
|   RSP0                 |  <- Stack pointer to use when transitioning from higher to ring 0
+------------------------+
|   RSP1                 |  <- Stack pointer for ring 1, rarely used
+------------------------+
|   RSP2                 |  <- Stack pointer for ring 2, rarely used
+------------------------+
|   Reserved             |
+------------------------+
|   IST1                 |  <- Interrupt Stack Table pointers (IST1 to IST7)
+------------------------+
|   IST2                 |
+------------------------+
|   ...                  |
+------------------------+
|   IST7                 |
+------------------------+
|   Reserved             |
+------------------------+
|   Reserved             |
+------------------------+
|   I/O Map Base Address |  <- Pointer to the I/O permission bitmap, rarely used
+------------------------+
|   ...                  |
+------------------------+
```

In the Interrupt Stack Table (IST), each entry holds a pointer to a dedicated stack that is used when a specific interrupt or exception occurs. This is especially important for handling certain types of exceptions like double fault or stack overflow, which may occur when the current stack is not available or already corrupted.

Using separate stacks for handling these critical exceptions can prevent a complete system failure when the current stack is in a bad state. The IST allows you to define up to 7 different stacks that can be used for different interrupts or exceptions.

However, note that managing these different stacks is complex, and requires a deep understanding of the interrupt handling mechanism and the specific needs of your operating system. 

As always, remember that directly interacting with the TSS is considered `unsafe` in Rust, because an incorrect configuration can lead to serious issues like system crashes. It's recommended to use provided abstractions whenever possible.


## How to Create a Task State Segment (TSS)?

To create a new TSS that holds a separate double fault stack in its interrupt stack table, you need to create a TSS struct. The x86_64 crate already includes a `TaskStateSegment` struct for this purpose. 

```rust
// in src/lib.rs
pub mod gdt;

// in src/gdt.rs
use x86_64::VirtAddr;
use x86_64::structures::tss::TaskStateSegment;
use lazy_static::lazy_static;

pub const DOUBLE_FAULT_IST_INDEX: u16 = 0;

lazy_static! {
    static ref TSS: TaskStateSegment = {
        let mut tss = TaskStateSegment::new();
        tss.interrupt_stack_table[DOUBLE_FAULT_IST_INDEX as usize] = {
            const STACK_SIZE: usize = 4096 * 5;
            static mut STACK: [u8; STACK_SIZE] = [0; STACK_SIZE];

            let stack_start = VirtAddr::from_ptr(unsafe { &STACK });
            let stack_end = stack_start + STACK_SIZE;
            stack_end
        };
        tss
    };
}
```

Here, we use lazy_static because Rust's const evaluator can't perform this initialization at compile time. We set the 0th IST entry as the double fault stack (though any other IST index would work as well). We then write the top address of a double fault stack to the 0th entry, as stacks on x86 grow downwards, i.e., from high addresses to low addresses.

## How to Load the Task State Segment (TSS)?

Once we have created the new TSS, we need to tell the CPU to use it. This can be somewhat complicated since the TSS uses the segmentation system due to historical reasons. Instead of loading the table directly, we need to add a new segment descriptor to the Global Descriptor Table (GDT). Then we can load our TSS by invoking the `ltr` instruction with the respective GDT index.

### What's the relationship between the IDT and TSS
For each type of interrupt, the IDT contains an entry which tells the processor where the Interrupt Service Routine (ISR) for that interrupt is located in memory.

The connection between the IDT and TSS comes in when an interrupt or exception occurs. When the processor receives an interrupt, it uses the IDT to find the appropriate ISR. If the interrupt is associated with a specific stack in the TSS through the IST, the processor will switch to that stack before invoking the ISR. This allows for better handling of certain critical exceptions that could occur when the current stack is in a bad state, like a stack overflow or a double fault.

### What's the GDT?
The Global Descriptor Table, or GDT, is a data structure used by Intel x86-family processors to define the characteristics of the various memory areas used during program execution, including the base address, the limit, and access privileges like executability and writability.

Each entry in the GDT is a data segment descriptor that describes the properties of a data segment. A segment, in this context, is a contiguous area of memory with specific access rights and characteristics. A descriptor contains information about the segment, such as:

- Base address: The starting address of the memory segment.
- Limit: The size of the segment. This is used to ensure that the program doesn't try to access memory outside of its allocated segment, which could potentially lead to issues such as overwriting critical system memory.
- Privilege level: This is a 2-bit value that determines the privilege level of the segment, with 0 being the highest (OS level) and 3 being the lowest (application level). This is used to implement protection and isolation between different privilege levels in the system.
- Descriptor type: This field defines whether the descriptor is for a data segment or a code segment. 
- Segment granularity: This specifies whether the limit field is interpreted in byte units or in page units.
- Other flags like accessed, readable (for code segments), or writable (for data segments), direction/conforming bits, etc.

The CPU uses the GDT through a special register, called the GDTR (Global Descriptor Table Register), which stores the size and starting address of the GDT. When the processor needs to access a memory segment (for example, to fetch, read, or write data), it uses a segment selector to index into the GDT and find the corresponding descriptor. The segment selector is typically stored in one of the segment registers (CS, DS, SS, ES, FS, GS in x86 architecture).

The GDT plays a crucial role in memory protection and system stability because it allows the operating system to control which parts of memory a process can access and what it can do with that access. This makes it much harder for a process to accidentally (or intentionally) interfere with the operation of other processes or the operating system itself. It is also used for implementing context switches, handling interrupts and exceptions, and performing system calls. 

In x86-64 (also known as AMD64) architecture, which is used in most modern PCs, the role of segmentation and hence GDT has been largely deprecated, and most of the memory management work is done using paging. However, the GDT is still required and used for certain specific tasks, such as holding the kernel's code and data segment descriptors, and the Task State Segment (TSS) descriptor, which is used for performing tasks like switching to a different stack when handling interrupts or exceptions.

## What are the final steps for handling double fault exceptions?

The final steps in properly handling double fault exceptions include reloading the code segment register, loading the TSS, and updating the IDT entry. This is necessary because the GDT segments aren't active yet as the segment and TSS registers still have values from the old GDT. Also, the double fault IDT entry should be modified to use the new stack.

## What's the relationship between the GDT and TSS

In x86_64 (or x86 in protected mode), the TSS must be defined in the GDT. This means there is an entry in the GDT that points to the base address of the TSS in memory. The CPU uses this GDT entry to locate and access the TSS when performing a task switch or handling an interrupt or exception.

When an interrupt or exception happens, the CPU loads the stack pointer and the stack segment from the TSS. This allows it to find where the interrupt or exception handler's stack is located.

### How to Reload the Code Segment Register and Load the TSS?

Reloading the code segment register is important after changing the GDT. This is because the old segment selector might now point to a different GDT descriptor, such as a TSS descriptor. 

Next, loading the TSS involves informing the CPU to use the TSS mentioned in the loaded GDT.

We need access to the `code_selector` and `tss_selector` variables in our `gdt::init` function to accomplish this. This is achieved by making them part of the static through a new `Selectors` struct:

```rust
// in src/gdt.rs

use x86_64::structures::gdt::SegmentSelector;

lazy_static! {
    static ref GDT: (GlobalDescriptorTable, Selectors) = {
        let mut gdt = GlobalDescriptorTable::new();
        let code_selector = gdt.add_entry(Descriptor::kernel_code_segment());
        let tss_selector = gdt.add_entry(Descriptor::tss_segment(&TSS));
        (gdt, Selectors { code_selector, tss_selector })
    };
}

struct Selectors {
    code_selector: SegmentSelector,
    tss_selector: SegmentSelector,
}
```

Now we can use the selectors to reload the `cs` register and load our TSS:

```rust
// in src/gdt.rs

pub fn init() {
    use x86_64::instructions::tables::load_tss;
    use x86_64::instructions::segmentation::{CS, Segment};
    
    GDT.0.load();
    unsafe {
        CS::set_reg(GDT.1.code_selector);
        load_tss(GDT.1.tss_selector);
    }
}
```

Here, we reload the code segment register using `CS::set_reg` and load the TSS using `load_tss`. These functions are marked as unsafe because loading invalid selectors could potentially break memory safety.

#### GDT.0.load
`GDT.0.load()` loads the Global Descriptor Table (GDT) into the CPU's GDTR (Global Descriptor Table Register).

The GDT is a table of segment descriptors, and it defines the different memory segments that the CPU can access, along with their permissions and other attributes. The GDTR is a special CPU register that stores the physical address of the GDT and the length of the table.

Here is what happens in detail when you call `GDT.0.load()`:

1. The `GlobalDescriptorTable::load()` function is called on the `GDT` object. This function is provided by the `x86_64` crate.

2. Inside the `load()` function, the address and size of the GDT are retrieved.

3. The `lgdt` instruction is used to load these values into the GDTR. This instruction is privileged and can only be used in kernel mode. The `lgdt` instruction takes a memory operand that points to a 6-byte pseudo-descriptor, which contains the limit (the length of the table minus 1) and the 32-bit linear base address of the GDT.

4. After the `lgdt` instruction is executed, the CPU will use the newly loaded GDT for all subsequent segment translations and checks.

This operation is marked as unsafe because if you load incorrect values into the GDTR, you could cause undefined behavior, such as segmentation faults or other memory errors. Therefore, it's very important that the GDT is set up correctly. 

#### CS::set
1. `GDT.1.code_selector` is a segment selector, which is a 16-bit value that specifies a descriptor in the GDT. In this case, it is the selector for the kernel's code segment, which was added to the GDT with `gdt.add_entry(Descriptor::kernel_code_segment())`.
    
2. `CS::set_reg()` is a function provided by the `x86_64` crate that sets the value of the CS register to the specified segment selector. This effectively tells the CPU that the code segment now starts at the base address and has the limit and access rights specified by the descriptor in the GDT that the selector points to.
    

This operation is marked `unsafe` because setting the CS register to an invalid selector could cause undefined behavior. The CS register is a privileged register that determines the memory segment where the CPU fetches its instructions from. If it points to a segment that does not contain valid code, or if it points to a segment that the program should not have access to (for example, a kernel segment from user space), the CPU could execute arbitrary data as code or violate the operating system's security boundaries.

### How to Update the IDT Entry?

After we have loaded a valid TSS and interrupt stack table, we can set the stack index for our double fault handler in the IDT:

```rust
// in src/interrupts.rs

use crate::gdt;

lazy_static! {
    static ref IDT: InterruptDescriptorTable = {
        let mut idt = InterruptDescriptorTable::new();
        idt.breakpoint.set_handler_fn(breakpoint_handler);
        unsafe {
            idt.double_fault.set_handler_fn(double_fault_handler)
                .set_stack_index(gdt::DOUBLE_FAULT_IST_INDEX); // new
        }

        idt
    };
}
```

The `set_stack_index` method is unsafe because the caller must ensure that the used index is valid and not already used for another exception.

After these steps, the CPU should switch to the double fault stack whenever a double fault occurs. This ensures we can catch all double faults, including kernel stack overflows. From now on, we should never see a triple fault again. As a good practice, it would be prudent to add a test to ensure these steps aren't accidentally broken.

## What is the Purpose of the Stack Overflow Test?

The Stack Overflow Test is used to test the new GDT module and ensure that the double fault handler is correctly called when a stack overflow occurs. The idea is to provoke a double fault in the test function and verify that the double fault handler is activated.

### How to Set Up a Minimal Skeleton for the Stack Overflow Test?

We can start with a simple skeleton like this:

```rust
// in tests/stack_overflow.rs

#![no_std]
#![no_main]

use core::panic::PanicInfo;

#[no_mangle]
pub extern "C" fn _start() -> ! {
    unimplemented!();
}

#[panic_handler]
fn panic(info: &PanicInfo) -> ! {
    blog_os::test_panic_handler(info)
}
```

This test will run without a test harness, similar to the `panic_handler` test, because we can't continue execution after a double fault. Therefore, running more than one test wouldn't be meaningful. We can disable the test harness for this test by adding the following to our `Cargo.toml`:

```toml
# in Cargo.toml

[[test]]
name = "stack_overflow"
harness = false
```

Now `cargo test --test stack_overflow` should compile successfully. However, the test will fail because the `unimplemented!()` macro will cause a panic.

### How to Implement the \_start Function in the Stack Overflow Test?

The `_start` function can be implemented as follows:

```rust
// in tests/stack_overflow.rs

use blog_os::serial_print;

#[no_mangle]
pub extern "C" fn _start() -> ! {
    serial_print!("stack_overflow::stack_overflow...\t");

    blog_os::gdt::init();
    init_test_idt();

    // trigger a stack overflow
    stack_overflow();

    panic!("Execution continued after stack overflow");
}

#[allow(unconditional_recursion)]
fn stack_overflow() {
    stack_overflow(); // for each recursion, the return address is pushed
    volatile::Volatile::new(0).read(); // prevent tail recursion optimizations
}
```

We first call the `gdt::init` function to initialize a new GDT. Instead of calling the `interrupts::init_idt` function, we call an `init_test_idt` function (explained in a later section) because we want to register a custom double fault handler. This custom handler should exit QEMU with a success code instead of panicking.

The `stack_overflow` function is used to trigger a stack overflow. It is almost identical to the function in our `main.rs` but includes an additional volatile read at the end to prevent a compiler optimization known as tail call elimination. This optimization can transform a function whose last statement is a recursive function call into a normal loop, thus keeping the stack usage constant. But for our test, we want a stack overflow to occur, so we add a dummy volatile read statement, which the compiler is not allowed to remove. Consequently, the function is no longer tail recursive, and the transformation into a loop is prevented. The `allow(unconditional_recursion)` attribute is also added to silence the compiler warning about the endless recursion of the function.

## What is the Purpose of the Test IDT in the Stack Overflow Test?

The Test IDT (Interrupt Descriptor Table) is used in the Stack Overflow Test to establish a custom double fault handler. This custom handler is necessary because, unlike in a regular execution context where a double fault would result in a panic, in the test context we want the double fault to exit QEMU with a success exit code.

### How is the Test IDT Implemented in the Stack Overflow Test?

The implementation of the Test IDT in the Stack Overflow Test is as follows:

```rust
// in tests/stack_overflow.rs

use lazy_static::lazy_static;
use x86_64::structures::idt::InterruptDescriptorTable;

lazy_static! {
    static ref TEST_IDT: InterruptDescriptorTable = {
        let mut idt = InterruptDescriptorTable::new();
        unsafe {
            idt.double_fault
                .set_handler_fn(test_double_fault_handler)
                .set_stack_index(blog_os::gdt::DOUBLE_FAULT_IST_INDEX);
        }

        idt
    };
}

pub fn init_test_idt() {
    TEST_IDT.load();
}
```

This implementation is similar to our normal IDT in `interrupts.rs`. We define a new `InterruptDescriptorTable` and set the double fault handler to `test_double_fault_handler`. As with the normal IDT, we set a stack index in the IST (Interrupt Stack Table) for the double fault handler, which ensures that we switch to a separate stack when a double fault occurs. The `init_test_idt` function loads the IDT on the CPU through the `load` method.

### How is the Custom Double Fault Handler Implemented in the Stack Overflow Test?

The custom double fault handler for the Stack Overflow Test is implemented as follows:

```rust
// in tests/stack_overflow.rs

use blog_os::{exit_qemu, QemuExitCode, serial_println};
use x86_64::structures::idt::InterruptStackFrame;

extern "x86-interrupt" fn test_double_fault_handler(
    _stack_frame: InterruptStackFrame,
    _error_code: u64,
) -> ! {
    serial_println!("[ok]");
    exit_qemu(QemuExitCode::Success);
    loop {}
}
```

In this function, when the double fault handler is called, we exit QEMU with a success exit code using `exit_qemu(QemuExitCode::Success)`, marking the test as passed. Because integration tests are completely separate executables, we need to set the `#![feature(abi_x86_interrupt)]` attribute again at the top of our test file.

Finally, we can run our test using `cargo test --test stack_overflow` (or `cargo test` to run all tests). As expected, we should see the `stack_overflow... [ok]` output in the console. If you try to comment out the `set_stack_index` line, it should cause the test to fail.