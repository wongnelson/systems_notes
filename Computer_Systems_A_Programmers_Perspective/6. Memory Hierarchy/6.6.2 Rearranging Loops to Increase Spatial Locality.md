## How can loops be rearranged to increase spatial locality?

Rearranging loops in a program can help increase spatial locality, resulting in better cache utilization and improved performance. Consider the problem of multiplying a pair of n Ã— n matrices, C = AB. A matrix multiplication function is usually implemented using three nested loops, which are identified by their indices i, j, and k. By permuting the loops and making some minor code changes, you can create six functionally equivalent versions of matrix multiplication.

### Example: Matrix Multiplication

Let's look at the six versions of matrix multiplication:

#### Version ijk
```c
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++) {
    sum = 0.0;
    for (k = 0; k < n; k++)
      sum += A[i][k]*B[k][j];
    C[i][j] += sum;
  }
```

#### Version jik
```c
for (j = 0; j < n; j++)
  for (i = 0; i < n; i++) {
    sum = 0.0;
    for (k = 0; k < n; k++)
      sum += A[i][k]*B[k][j];
    C[i][j] += sum;
  }
```

#### Version jki
```c
for (j = 0; j < n; j++)
  for (k = 0; k < n; k++) {
    r = B[k][j];
    for (i = 0; i < n; i++)
      C[i][j] += A[i][k]*r;
  }
```

#### Version kji
```c
for (k = 0; k < n; k++)
  for (j = 0; j < n; j++) {
    r = B[k][j];
    for (i = 0; i < n; i++)
      C[i][j] += A[i][k]*r;
  }
```

#### Version kij

```c
for (k = 0; k < n; k++)
  for (i = 0; i < n; i++) {
    r = A[i][k];
    for (j = 0; j < n; j++)
      C[i][j] += r*B[k][j];
  }
```

#### Version ikj
```c
for (i = 0; i < n; i++)
  for (k = 0; k < n; k++) {
    r = A[i][k];
    for (j = 0; j < n; j++)
      C[i][j] += r*B[k][j];
  }
```

Each version has a unique ordering of its loops. The six versions can be grouped into three equivalence classes, denoted by the pair of matrices accessed in the inner loop. The classes are AB (versions ijk and jik), AC (versions jki and kji), and BC (versions kij and ikj). Each class has different cache access patterns and miss rates, leading to varying performance.

## How can the matrix multiplication example demonstrate the impact of loop rearrangement on spatial locality?

Analyzing the inner loop behavior of the six matrix multiplication versions, you can observe differences in the number of accesses and the locality. The inner loops of the class AB routines (versions ijk and jik) scan a row of array A with a stride of 1, while the inner loop scans a column of B with a stride of n. The miss rate for A is 0 .25 misses per iteration, and for B, it is 1 miss per iteration, totaling 1.25 misses per iteration.

The class AC routines (versions jki and kji) have some issues. Each iteration performs two loads and a store, compared to the class AB routines, which perform two loads and no stores. The inner loop scans the columns of A and C with a stride of n, resulting in a miss on each load and a total of 2 misses per iteration. Interchanging the loops in the AC routines has decreased the spatial locality compared to the AB routines.

The class BC routines (versions kij and ikj) present an interesting trade-off. They require one more memory operation than the AB routines, with two loads and a store. However, since the inner loop scans both B and C row-wise with a stride-1 access pattern, the miss rate on each array is only 0.25 misses per iteration, totaling 0.50 misses per iteration.

In a performance analysis, the fastest version (kij and ikj) runs almost 40 times faster than the slowest version for large values of n, even though each performs the same number of floating-point arithmetic operations. Pairs of versions with the same number of memory references and misses per iteration have almost identical measured performance.

Miss rate is a better predictor of performance than the total number of memory accesses. For example, the class BC routines, with 0.5 misses per iteration, perform much better than the class AB routines, with 1.25 misses per iteration, even though the class BC routines perform more memory operations.

For large values of n, the performance of the fastest pair of versions (kij and ikj) is constant. Even though the array is much larger than any of the SRAM cache memories, the prefetching hardware is smart enough to recognize the stride-1 access pattern and fast enough to keep up with memory accesses in the tight inner loop. This highlights the importance of developing programs with good spatial locality.

### Why are kij and ikj the fastest?

The kij and ikj versions of matrix multiplication are the fastest because of the spatial locality they exhibit in memory accesses. Spatial locality refers to the likelihood that nearby memory locations will be accessed together in a short period of time. When a program exhibits good spatial locality, it can take advantage of the cache efficiently, leading to improved performance.

In the kij and ikj versions, the inner loop scans both matrices B and C row-wise with a stride-1 access pattern. This means that the elements in B and C are accessed consecutively, which is a more cache-friendly pattern. When a cache block is fetched from memory, it typically contains several neighboring elements. As a result, the stride-1 access pattern allows the program to make better use of the cache by minimizing cache misses.

Here's a more detailed breakdown of the kij and ikj versions' behavior:

1.  **Access pattern:** Both kij and ikj access the elements of matrices B and C row-wise in their inner loops. The elements in a row are stored in adjacent memory locations, making the access pattern cache-friendly.
    
2.  **Cache block utilization:** In these versions, each cache block fetched from memory is used more effectively. The cache block contains several consecutive elements from the row, which are likely to be used in the upcoming iterations. This leads to fewer cache misses and better cache utilization.
    
3.  **Miss rate:** As a result of the stride-1 access pattern, the miss rate on each array (B and C) is only 0.25 misses per iteration, totaling 0.50 misses per iteration. This is lower than the miss rates in the other versions, which directly affects the overall performance of the matrix multiplication.
    
4.  **Prefetching hardware:** Modern processors have prefetching hardware that can detect stride-1 access patterns and automatically fetch the required cache blocks ahead of time. This helps in hiding the memory access latency, further improving the performance of the kij and ikj versions.


### What is blocking?

Blocking is a technique used to improve the temporal locality of inner loops in a program. Temporal locality refers to the tendency of a program to access the same memory locations repeatedly within a short time frame. By improving temporal locality, a program can make better use of the cache, leading to improved performance.

### How does blocking work?

Blocking works by organizing the data structures in a program into large chunks called blocks. In this context, a "block" refers to an application-level chunk of data, not a cache block. The program is structured in such a way that it loads a block into the L1 cache, performs all the necessary reads and writes on that block, discards the block, loads the next block, and so on.

### When should blocking be used?

Blocking is best suited for optimizing compilers or frequently executed library routines. This is because it often makes the code harder to read and understand. While blocking may not always improve performance, it is still an important technique to study and understand, as it can produce significant performance gains on systems that do not have sophisticated prefetching hardware.

### How can blocking improve the performance of matrix multiplication?

Consider a matrix multiplication problem, C = A * B, where A, B, and C are n x n matrices. The traditional approach consists of three nested loops, each iterating over the rows and columns of the matrices. By applying blocking, we can divide the matrices into smaller submatrices (or blocks) and perform the multiplication on these smaller submatrices. This approach can improve the temporal locality by allowing the smaller submatrices to fit within the cache, reducing cache misses and improving performance.

Here's an example of how blocking can be applied to matrix multiplication:

```c
void block_matrix_multiply(double **A, double **B, double **C, int n, int blockSize) {
  for (int i = 0; i < n; i += blockSize) {
    for (int j = 0; j < n; j += blockSize) {
      for (int k = 0; k < n; k += blockSize) {
        // Multiply the submatrices
        for (int ii = i; ii < i + blockSize; ii++) {
          for (int jj = j; jj < j + blockSize; jj++) {
            for (int kk = k; kk < k + blockSize; kk++) {
              C[ii][jj] += A[ii][kk] * B[kk][jj];
            }
          }
        }
      }
    }
  }
}
```
In this example, the matrices are divided into smaller submatrices of size `blockSize` x `blockSize`. The outer loops iterate over the blocks, while the inner loops perform the actual multiplication on the smaller submatrices.

Keep in mind that blocking does not always guarantee performance improvements. Its effectiveness depends on various factors, such as the cache size, block size, and the prefetching hardware of the system.