```toc
```
## What is a cache and what is the process of using a cache called?

A cache is a small, fast storage device that acts as a staging area for data objects stored in a larger, slower device. The process of using a cache is known as caching.

## What is the central idea of a memory hierarchy?

The central idea of a memory hierarchy is that for each level k, the faster and smaller storage device at level k serves as a cache for the larger and slower storage device at level k + 1. Each level in the hierarchy caches data objects from the next lower level.

## How are storage devices at each level of the memory hierarchy partitioned?

The storage devices at each level of the memory hierarchy are partitioned into contiguous chunks of data objects called blocks. Each block has a unique address or name that distinguishes it from other blocks.

An example of the memory hierarchy with partitioned blocks can be demonstrated as follows:

1.  Registers: The smallest and fastest level of memory in a computer system is the register. Each register is a small, fast storage device that holds a single data object or instruction. Each register has a unique address, and the contents of the register can be retrieved or written in a single clock cycle.
    
2.  Cache: The cache is a small, fast memory that acts as a buffer between the processor and main memory. The cache is partitioned into blocks, each of which holds multiple data objects. Each block has a unique address or tag that distinguishes it from other blocks in the cache. When the processor needs to access data, it first checks the cache to see if the data is present in a block. If the data is present, the processor retrieves it from the cache, avoiding the slower main memory access.
```c
+-----------------+               +---------------------+
|    Processor    |               |     Main Memory     |
+-----------------+               +---------------------+
        |                                 |
        v                                 v
+-----------------+     +---------------------+
|      Cache      |<----|   Data from memory   |
+-----------------+     +---------------------+
|    Block 1      |     | Data Object 1 | ... |
|  Unique Tag 1   |     | Data Object 2 | ... |
|  Data Object 1  |     | Data Object 3 | ... |
|  Data Object 2  |     | Data Object 4 | ... |
|       ...       |     |       ...      | ... |
|    Block n      |     | Data Object 1 | ... |
| Unique Tag n    |     | Data Object 2 | ... |
|  Data Object 1  |     | Data Object 3 | ... |
|  Data Object 2  |     | Data Object 4 | ... |
|       ...       |     |       ...      | ... |
+-----------------+     +---------------------+
```
    
3.  Main Memory: Main memory is a large, slower memory that stores all the data and instructions that the processor needs to access. Main memory is also partitioned into blocks, each of which holds multiple data objects. Each block has a unique address or name that distinguishes it from other blocks in main memory. When the processor needs to access data that is not present in the cache, it retrieves it from main memory.
    
4.  Secondary Storage: Secondary storage is the largest, slowest memory in a computer system. It stores data and programs that are not currently in use by the processor. Secondary storage is partitioned into blocks, each of which holds multiple data objects. Each block has a unique address or name that distinguishes it from other blocks in secondary storage. When the processor needs to access data that is not present in main memory, it retrieves it from secondary storage.
    

In this example, each level of the memory hierarchy is partitioned into blocks, and each block has a unique address or name that distinguishes it from other blocks. This partitioning allows for efficient access to data and improves performance by reducing the number of memory accesses required to retrieve data.

## Cache Blocks 

## What's a usecase where knowing about cache blocks is useful?
Let's say you're working on optimizing a video editing application that needs to read large amounts of data from memory. By analyzing the reference patterns of the application, you determine that it has a reference pattern characterized by sequential access to large blocks of data.

Knowing this, you can optimize the application's cache usage by increasing the cache block size. This allows the cache to store more data and reduces the number of cache misses, which in turn improves the overall performance of the application.

On the other hand, if the reference pattern was characterized by random access to small blocks of data, you might want to decrease the cache block size to reduce the overhead of cache management and improve performance.

### What is a cache block?

A cache block is a unit of data storage in cache memory.

### What does a cache block consist of?

A cache block typically consists of multiple bytes, such as 64 bytes or 128 bytes.

### What is the purpose of a cache block?

Cache blocks are used to store frequently accessed data from main memory so that it can be quickly accessed by the CPU without having to wait for the slower main memory.

### How does the size of a cache block affect cache performance?

The size of a cache block, also known as the cache line size, can have a significant impact on cache performance. If the block size is too small, it can result in a high number of cache misses and increased memory access time. On the other hand, if the block size is too large, it can lead to a waste of cache space and an increase in cache pollution.

In general, a larger cache block size can reduce the number of cache misses and improve cache performance for programs with reference patterns that exhibit spatial locality, where nearby data is accessed together. This is because a larger block size can store more data that is likely to be accessed together, reducing the need for additional memory access.

However, a smaller cache block size may be more effective for programs with reference patterns that exhibit temporal locality, where the same data is accessed repeatedly over a short period of time. This is because a smaller block size can store fewer data that is more likely to be accessed repeatedly, reducing the need to fetch data from memory repeatedly.

## What is a cache hit?

A cache hit occurs when a program needs a particular data object d from level k + 1, and d happens to be cached at level k. The program reads d directly from level k, which is faster than reading d from level k + 1.

## What is a cache miss?

A cache miss occurs when the data object d is not cached at level k. In this case, the cache at level k fetches the block containing d from the cache at level k + 1, possibly overwriting an existing block if the level k cache is already full.

## What are the different kinds of cache misses?

There are three main kinds of cache misses:

1.  Compulsory misses or cold misses: Occur when the cache at level k is empty.

A scenario that demonstrates a compulsory miss or cold miss is as follows:

1.  The processor is powered on and the cache at level k is empty.
2.  The processor attempts to access data from the cache, but since the cache is empty, it cannot find the data in the cache.
3.  The processor generates a cache miss and requests the data from the next level of the memory hierarchy, such as main memory.
4.  The data is retrieved from main memory and loaded into the cache at level k.
5.  The processor retrieves the data from the cache and continues processing.

In this scenario, the cache miss is a compulsory miss or cold miss because the cache is empty and there is no opportunity to retrieve the data from the cache. The processor must request the data from the next level of the memory hierarchy and load it into the cache before it can be used. This type of miss occurs when the cache is initially empty or when the data has been evicted from the cache and has not been accessed again.

2.  Conflict misses: Occur when the cache is large enough to hold the referenced data objects, but because they map to the same cache block, the cache keeps missing.
A scenario that demonstrates a conflict miss is as follows:

1.  <mark style="background: #FFB8EBA6;">The processor is executing a program that accesses multiple data objects with the same size.</mark>
2.  The cache at level k is large enough to hold the referenced data objects.
3.  The data objects map to the same cache block, which means that the cache can only store one of the data objects at a time.
4.  The processor accesses the first data object and retrieves it from the cache.
5.  The processor accesses the second data object, but since it maps to the same cache block as the first data object, the cache must evict the first data object to make room for the second data object.
6.  The processor generates a cache miss and retrieves the second data object from main memory.
7.  The processor accesses the first data object again, but since it has been evicted from the cache, it generates another cache miss and retrieves it from main memory again.

In this scenario, the cache misses are conflict misses because the cache is large enough to hold the referenced data objects, but they map to the same cache block and cannot be stored in the cache simultaneously. The cache must constantly evict and retrieve the data objects, resulting in multiple cache misses. Conflict misses occur when the cache is not large enough to hold all the data objects that are being accessed or when the data objects have a high degree of temporal or spatial locality.

<mark style="background: #FFB8EBA6;">Here's an example:</mark>

1.  The processor is executing a program that accesses two arrays `A` and `B` with the same size of 10 elements each.
2.  The cache block size is 5 elements.
3.  The first 5 elements of array `A` map to the first cache block.
4.  The first 5 elements of array `B` also map to the first cache block.
5.  The processor accesses the first 5 elements of array `A` and retrieves them from the cache.
6.  The processor accesses the first 5 elements of array `B`, but since they map to the same cache block as the first 5 elements of array `A`, the cache must evict the first 5 elements of array `A` to make room for the first 5 elements of array `B`.
7.  The processor generates a cache miss and retrieves the first 5 elements of array `B` from main memory.
8.  The processor accesses the first 5 elements of array `A` again, but since they have been evicted from the cache, it generates another cache miss and retrieves them from main memory again.

In this example, the cache misses are conflict misses because the first 5 elements of arrays `A` and `B` map to the same cache block and cannot be stored in the cache simultaneously. The cache must constantly evict and retrieve the data objects, resulting in multiple cache misses. Conflict misses occur when the cache is not large enough to hold all the data objects that are being accessed or when the data objects have a high degree of temporal or spatial locality.


3.  Capacity misses: Occur when the size of the working set exceeds the size of the cache.'
Here's an example:

1.  The processor is executing a program that accesses an array `A` with a size of 100 elements.
2.  The cache at level k has a capacity of 50 elements.
3.  The processor accesses the first 50 elements of array `A` and retrieves them from main memory and stores them in the cache.
4.  The processor accesses the next 50 elements of array `A`, but since the cache only has a capacity of 50 elements, it cannot hold all 100 elements of the array `A`.
5.  The cache generates a capacity miss and retrieves the next 50 elements of array `A` from main memory.'

## How is cache management performed at each level of the memory hierarchy?

Cache management at each level of the memory hierarchy can be done by hardware, software, or a combination of the two. For example, the compiler manages the register file, hardware logic built into the caches manages levels L1, L2, and L3, and a combination of operating system software and address translation hardware on the CPU manages the DRAM main memory.

## What is a replacement policy?

A replacement policy is a set of rules that a cache follows to decide which block to replace or evict when fetching a new block from the next lower level in the memory hierarchy. This is done when the cache is already full, and there is no space for the new block.

## What are some examples of replacement policies?

Two examples of replacement policies are:

1.  Random replacement policy: Chooses a random victim block to be replaced.
2.  Least Recently Used (LRU) replacement policy: Chooses the block that was last accessed the furthest in the past to be replaced.

## What is a working set?

A working set is a reasonably constant set of cache blocks accessed by a program during a particular phase (e.g., loops). Programs often run as a sequence of phases where each phase accesses a specific working set.

## What is the role of block sizes in the memory hierarchy?

Block sizes play a crucial role in transferring data between adjacent levels in the memory hierarchy. While the block size is fixed between any particular pair of adjacent levels in the hierarchy, other pairs of levels can have different block sizes. Devices lower in the hierarchy (further from the CPU) have longer access times and tend to use larger block sizes to amortize these longer access times.

## How does a placement policy affect cache misses?

A placement policy determines where to place the block fetched from level k + 1 in level k. A flexible placement policy allows any block from level k + 1 to be stored in any block at level k, while a restrictive placement policy limits the possible locations. Restrictive placement policies can lead to conflict misses, where the cache is large enough to hold the referenced data objects, but they map to the same cache block, causing misses.

## How are caches managed in a system with virtual memory?

In a system with virtual memory, the DRAM main memory serves as a cache for data blocks stored on disk and is managed by a combination of operating system software and address translation hardware on the CPU.

## What are some examples of caching in modern computer systems?

Some examples of caching in modern computer systems include CPU registers, translation lookaside buffer (TLB), L1, L2, and L3 caches, virtual memory, buffer cache, disk cache, network cache, browser cache, and web cache.

Type | What cached | Where cached | Latency (cycles) | Managed by
----- | ---------- | ----------- | --------------- | ---------
CPU registers | 4-byte or 8-byte words | On-chip CPU registers | 0 | Compiler
TLB | Address translations | On-chip TLB | 0 | Hardware MMU
L1 cache | 64-byte blocks | On-chip L1 cache | 4 | Hardware
L2 cache | 64-byte blocks | On-chip L2 cache | 10 | Hardware
L3 cache | 64-byte blocks | On-chip L3 cache | 50 | Hardware
Virtual memory | 4-KB pages | Main memory | 200 | Hardware + OS
Buffer cache | Parts of files | Main memory | 200 | OS
Disk cache | Disk sectors | Disk controller | 100,000 | Controller firmware
Network cache | Parts of files | Local disk | 10,000,000 | NFS client
Browser cache | Web pages | Local disk | 10,000,000 | Web browser
Web cache | Web pages | Remote server disks | 1,000,000,000 | Web proxy server
