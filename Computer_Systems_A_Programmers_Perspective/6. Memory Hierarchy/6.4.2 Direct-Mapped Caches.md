## What are direct-mapped caches?

Direct-mapped caches are a class of caches that have exactly one line per set (E = 1). They are the simplest type of caches to implement and understand, making them useful for illustrating general concepts about how caches work.

## What are the three main steps involved in cache operation when the CPU reads a memory word?

When the CPU reads a memory word, the cache goes through three main steps:

1.  Set selection
2.  Line matching
3.  Word extraction

These steps help the cache determine whether a request is a hit or a miss and then extract the requested word.

## How does set selection work in direct-mapped caches?

Set selection is a process used in computer architecture and cache memory design. It is the process of determining which cache set a particular memory block should be stored in.

In set selection, the cache extracts the s set index bits from the <mark style="background: #FFB8EBA6;">middle of the address </mark>for the memory word. These bits are interpreted as an unsigned integer that corresponds to a set number. If the cache is thought of as a one-dimensional array of sets, then the set index bits form an index into this array. For example, if the set index bits are 00001 (binary), it would select set 1.

<mark style="background: #FFB8EBA6;">pink</mark>
The reason for extracting the set index bits from the middle of the address for the memory word is to ensure that the cache is evenly distributed across all sets. The middle of the address provides a good balance between using too few bits for the set index, which would result in too few sets and increased competition for cache lines, and using too many bits, which would result in too many sets and reduced efficiency. By extracting the set index bits from the middle of the address, the cache can achieve a good balance between these two factors and provide a well-distributed, efficient cache.

## How does line matching work in direct-mapped caches?

In line matching, the cache determines if a copy of the word is stored in the cache line contained in the selected set. In a direct-mapped cache, this is easy and fast because there is exactly one line per set. A copy of the word is contained in the line if and only if the valid bit is set and the tag in the cache line matches the tag in the address of the word.

## How does word selection work in direct-mapped caches?

Once the cache determines there is a hit, it knows that the desired word is somewhere in the block. The word selection step determines where the desired word starts in the block. The block offset bits provide the offset of the first byte in the desired word. Similar to thinking of a cache as an array of lines, a block can be thought of as an array of bytes, and the byte offset as an index into that array.

```c
Cache Block
---------------------
| Byte 0 | Byte 1 | Byte 2 | Byte 3 |
---------------------

Byte Offsets
  0        1        2        3

Desired Word: Byte 1
      |
      v
---------------------
| Byte 0 | Byte 1 | Byte 2 | Byte 3 |
---------------------
```
In this example, the cache block contains 4 bytes (Byte 0, Byte 1, Byte 2, Byte 3). To select the desired word (in this case, Byte 1), we use the byte offset as an index into the array of bytes within the cache block.

## How does line replacement on misses work in direct-mapped caches?

If the cache misses, it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block in one of the cache lines of the set indicated by the set index bits. If the set is full of valid cache lines, one of the existing lines must be evicted. For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial: the current line is replaced by the newly fetched line.

Example Initial Cache State:
```c
Cache:
-------------
| Set 0 | Set 1 |
|-------|-------|
| Line  | Line  |
| (V)   | (V)   |
-------------

Memory Hierarchy:
------------------------------------------
| Block A | Block B | Block C | Block D |
------------------------------------------
```
Cache miss: Block B needed, maps to Set 0
```c
Cache:
-------------
| Set 0 | Set 1 |
|-------|-------|
| Line  | Line  |
| (V)   | (V)   |
-------------

Memory Hierarchy:
------------------------------------------
| Block A | *Block B* | Block C | Block D |
------------------------------------------
```
Cache after block retrieval and replacement in Set 0:
```c
Cache:
--------------------------------
| *Set 0 (Block B)* | Set 1     |
|-------------------|-----------|
| Line              | Line      |
| (V)               | (V)       |
--------------------------------
```
In this example, the cache has 2 sets, and each set has 1 line. The cache miss occurs because Block B is needed and it maps to Set 0. Since it's a direct-mapped cache, there's only one line per set, and the current line in Set 0 is replaced with the newly fetched block (Block B). The asterisks (*) in the diagrams indicate the block and set involved in the cache miss and replacement process.

### Enumerating address space

Enumerating the entire address space and partitioning the bits helps in understanding the cache mechanism. For a 4-bit address space, the following observations can be made:

1.  The concatenation of the tag and index bits uniquely identifies each block in memory.
2.  Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set.
3.  Blocks that map to the same cache set are uniquely identified by the tag.

### Simulating cache in action

To understand how a direct-mapped cache works in action, consider simulating a sequence of CPU reads:

1.  Read word at address 0 - cache miss, fetch block 0 from memory, and store it in set 0.
2.  Read word at address 1 - cache hit, return the value from block[1] of the cache line.
3.  Read word at address 13 - cache miss, load block 6 into set 2, and return the value from block[1] of the new cache line.
4.  Read word at address 8 - cache miss (due to tag mismatch), load block 4 into set 0, replacing the previous line.
5.  Read word at address 0 - another cache miss (conflict miss) due to the replacement during the previous read.

## What are conflict misses in direct-mapped caches?

Conflict misses occur when programs access arrays whose sizes are a power of 2. In a direct-mapped cache, this often results in thrashing, where the cache repeatedly loads and evicts the same sets of cache blocks. Thrashing can cause significant performance problems and slowdowns.

Here's an example illustrating conflict misses and thrashing in a direct-mapped cache with 4 sets (Set 0 to Set 3) and 1 line per set. Let's assume the program is accessing an array of size 16 (2^4) whose elements are stored in memory blocks A, B, C, D, E, F, G, and H. Each block maps to the cache sets as follows: A and E map to Set 0, B and F map to Set 1, C and G map to Set 2, and D and H map to Set 3.

Initial Cache State (empty):
```c
Cache:
---------------------------------
| Set 0 | Set 1 | Set 2 | Set 3 |
|-------|-------|-------|-------|
|       |       |       |       |
---------------------------------
```
The program accesses the array in the order A, B, C, D, E, F, G, H, A, B, C, D, E, F, G, H.

After accessing A, B, C, and D:
```c
Cache:
--------------------------
| A     | B     | C     | D     |
| (V)   | (V)   | (V)   | (V)   |
--------------------------
```

Accessing E (conflict miss, replaces A in Set 0):
```c
Cache:
--------------------------
| E     | B     | C     | D     |
| (V)   | (V)   | (V)   | (V)   |
--------------------------
```
Accessing F (conflict miss, replaces B in Set 1):
```c
Cache:
--------------------------
| E     | F     | C     | D     |
| (V)   | (V)   | (V)   | (V)   |
--------------------------
```
Accessing G and H (similar conflict misses):
```c
Cache:
--------------------------
| E     | F     | G     | H     |
| (V)   | (V)   | (V)   | (V)   |
--------------------------
```
Accessing A again (conflict miss, replaces E in Set 0):
```c
Cache:
--------------------------
| A     | F     | G     | H     |
| (V)   | (V)   | (V)   | (V)   |
--------------------------
```

As the program continues to access the array elements in the same order, the cache experiences conflict misses and thrashing as blocks are repeatedly loaded and evicted. This results in a significant performance loss and slowdowns.

### How can thrashing be fixed?

Thrashing can be fixed by adding padding to the arrays. In this example, let's assume we have two arrays, x and y, both with 8 elements. The program accesses the arrays in the order x[0], y[0], x[1], y[1], x[2], y[2], ..., x[7], y[7]. The cache has 4 sets (Set 0 to Set 3) and 1 line per set. Each element of x and y maps to the same set, causing conflict misses.

Initial Cache State (empty):
```c
Cache:
---------------------------------
| Set 0 | Set 1 | Set 2 | Set 3 |
|-------|-------|-------|-------|
|       |       |       |       |
---------------------------------
```
Accessing x[0] and y[0] (mapping to Set 0):
```c
Cache:
---------------------------------
| x[0]  |       |       |       |
| (V)   |       |       |       |
---------------------------------

Cache:
---------------------------------
| y[0]  |       |       |       |
| (V)   |       |       |       |
---------------------------------
```
Continuing this pattern results in conflict misses for each pair of x[i] and y[i].

Now, let's fix the issue by adding padding to array x, making it `float x[12]` instead of `float x[8]`. This changes the mapping of x and y elements to different sets in the cache.

Accessing x[0] and y[0] (now mapping to different sets):
```c
Cache:
---------------------------------
| x[0]  | y[0]  |       |       |
| (V)   | (V)   |       |       |
---------------------------------
```
Now, both x[i] and y[i] can be in the cache simultaneously, eliminating thrashing and conflict misses. As the program continues to access the elements, it will not experience the same level of conflict misses as before, improving performance.

## 6.11
1.  Imagine a hypothetical cache that uses the high-order s bits of an address as the set index. For such a cache, contiguous chunks of memory blocks are mapped to the same cache set.
    
    1.  How many blocks are in each of these contiguous array chunks?
        
    2.  Consider the following code that runs on a system with a cache of the form _(S, E, B, m)_ = (512, 1, 32, 32):
        
        ```
        
        int array[4096];
        for (i = 0; i < 4096; i++)
        	sum += array [i];
        ```
        
        What is the maximum number of array blocks that are stored in the cache at any point in time?

### How many blocks are in each of these contiguous array chunks?
With high-order bit indexing, the cache memory is organized into S = 2^s cache sets, with each set consisting of E cache lines. The set index is determined by using the s high-order bits of the memory address of a block. This means that blocks with contiguous memory addresses will map to the same set if the high-order s bits are the same.

For example, consider a cache with (S, E, B, m) = (512, 1, 32, 32), which means that s = 9 and t = 32 - (9 + 5) = 18. For this cache, the first 2^9 = 512 contiguous blocks of memory would map to set 0, the next 512 contiguous blocks would map to set 1, and so on.

Each contiguous chunk of memory blocks will consist of 2^t blocks, where t is the number of tag bits. In this example, each contiguous chunk will consist of 2^18 = 262,144 blocks. The first 2^18 contiguous blocks of the array would map to set 0, the next 2^18 blocks would map to set 1, and so on.

So, the maximum number of blocks that can be stored in the cache at any point in time is determined by the number of cache lines in each set, E, and the number of sets, S. In this example, since E = 1 and S = 512, the maximum number of blocks that can be stored in the cache at any point in time is 1 * 512 = 512.

### Second question's answer

The direct-mapped cache with the parameters (S, E, B, m) = (512, 1, 32, 32) has a cache capacity of 512 32-byte blocks. Each cache line in the direct-mapped cache contains a data block of 32 bytes, a valid bit to indicate if the line contains meaningful information, and t = 32 - (5 + 9) = 18 tag bits, which are a subset of the bits from the current block's memory address, to uniquely identify the block stored in the cache line.

In this particular case, the code accesses an array of size 4096, which is small enough to fit entirely in the cache. However, because the cache uses high-order bit indexing, the blocks of the array are not distributed evenly across the cache sets. Instead, the first 218 blocks in the array are mapped to set 0, the next 218 blocks to set 1, and so on. This means that the cache can hold at most 1 array block at any point in time, and all other blocks in the array will be evicted from the cache.

Therefore, this direct-mapped cache with high-order bit indexing makes poor use of the cache. The limited size of the cache sets and the eviction of blocks from the cache result in frequent cache misses, and the processor has to access the slower main memory to retrieve the data.

