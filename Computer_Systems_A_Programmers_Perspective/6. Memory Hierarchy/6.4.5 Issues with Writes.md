## What are the main issues with cache writes?

Cache writes are more complicated than reads because t<mark style="background: #FFB8EBA6;">hey must handle the updates to the cache and the next lower level of the memory hierarchy</mark>. There are two main issues with cache writes: how to update the next lower level of the memory hierarchy after a write hit and how to deal with write misses.

<mark style="background: #FFB8EBA6;">pink</mark>
Cache writes handle updates to the next lower level of the memory hierarchy because the cache is only a temporary, faster storage location that holds a subset of data from the lower-level memory. When data is modified in the cache, it is necessary to ensure consistency between the cache and the lower-level memory to maintain the correctness of the overall system state.

If the updated data in the cache is not propagated to the lower-level memory and the cache line is later evicted or replaced, the lower-level memory will still hold the old, outdated data. This inconsistency can lead to incorrect results, data corruption, or other unexpected behavior in a system.

By handling updates to the next lower level of the memory hierarchy, the cache ensures that the most recent version of the data is available throughout the entire memory system. Different approaches, such as write-through and write-back, are used to manage this process, with each having its own trade-offs in terms of performance, complexity, and bus traffic.

## What is the write-through approach for updating the next lower level of the memory hierarchy after a write hit?

The write-through approach immediately writes the updated cache block to the next lower level after a write hit. This approach is simple but causes bus traffic with every write, which can be a disadvantage.
```
  CPU
   |
   v
┌───────┐    Write hit       ┌───────┐  Write-through  ┌─────────┐
| Cache |<-------------------| Bus   |<--------------->| Lower   |
|       |   (Update cache)   |       | (Update lower)  | Level   |
|       |                    |       |   Memory        | Memory  |
└───────┘                    └───────┘                 └─────────┘

```
## What is the write-back approach for updating the next lower level of the memory hierarchy after a write hit?

The write-back approach defers the update as long as possible by writing the updated block to the next lower level only when it is evicted from the cache by the replacement algorithm. Write-back can significantly reduce the amount of bus traffic but adds complexity because the cache must maintain an additional dirty bit for each cache line that indicates whether or not the cache block has been modified.
```c
  CPU
   |
   v
┌───────┐    Write hit       ┌───────┐                ┌─────────┐
| Cache |<------------------>| Bus   |                | Lower   |
|       |   (Update cache)   |       |                | Level   |
|       |   (Set dirty bit)  |       |                | Memory  |
└───────┘                    └───────┘                └─────────┘
        \
         \
          Eviction & Write-back (when needed)
           \
            v
┌──────────────────────────────────────────────┐
| Replacement Algorithm (Choose line to evict) |
└──────────────────────────────────────────────┘
              |
              |
              v
           ┌───────┐    Write-back (deferred)    ┌─────────┐
           | Cache |<--------------------------->| Lower   |
           |       | (Update lower-level memory) | Level   |
           |       | (if dirty bit is set)       | Memory  |
           └───────┘                             └─────────┘

```
The deferment of the write-back operation is represented by an additional connection between the cache and the lower-level memory. This connection only takes place when the replacement algorithm evicts a cache line, and the dirty bit is set, indicating that the cache block has been modified.
## How does the write-allocate approach deal with write misses?

The write-allocate approach is a technique used in cache design that is intended to improve performance for write operations by exploiting spatial locality, which is the tendency of programs to write to adjacent memory locations over short periods of time.

When a write operation misses in the cache, the write-allocate approach loads an entire block of memory from the next lower level of the memory hierarchy into the cache, even if only one word or byte in the block is being written. This is done in order to take advantage of the spatial locality of writes, since it is likely that the adjacent words or bytes in the same block will be written to soon.

However, the disadvantage of the write-allocate approach is that every write miss results in a block transfer from the next lower level to the cache, which can be a significant overhead in terms of time and memory bandwidth. This can result in reduced performance and increased memory traffic, especially in cases where the working set of the application is larger than the cache size, or if there are frequent write misses. To mitigate this issue, some cache designs use a write-no-allocate approach.

## How does the no-write-allocate approach deal with write misses?

The no-write-allocate approach bypasses the cache and writes the word directly to the next lower level when dealing with write misses. Write-through caches typically use the no-write-allocate approach, while write-back caches typically use the write-allocate approach.

## Why is it recommended to adopt a mental model that assumes write-back, write-allocate caches when writing cache-friendly programs?

Assuming a write-back, write-allocate approach matches current trends as logic densities increase, making the increased complexity of write-back less of an impediment, and this approach is seen in all levels of modern systems. Furthermore, write-back, write-allocate is symmetric to the way reads are handled, which allows developers to focus on optimizing programs for good spatial and temporal locality instead of optimizing for a specific memory system.