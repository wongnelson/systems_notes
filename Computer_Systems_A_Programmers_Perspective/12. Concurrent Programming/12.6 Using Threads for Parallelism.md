## What is the relationship between sequential, concurrent, and parallel programs?

Sequential programs are written as a single logical flow, while concurrent programs are written as multiple concurrent flows. Parallel programs are concurrent programs running on multiple processors. The set of parallel programs is a proper subset of the set of concurrent programs.

## Why is exploiting parallelism important in modern applications?

Exploiting parallelism is critically important in applications such as busy Web servers, database servers, and large scientific codes. It is also becoming increasingly useful in mainstream applications like Web browsers, spreadsheets, and document processors. Parallelism helps these applications run faster on multi-core processor systems, as concurrent threads can be scheduled in parallel on multiple cores, rather than sequentially on a single core.

## How can we sum the sequence of integers 0, ..., n âˆ’ 1 in parallel?

We can sum the sequence by partitioning it into `t` disjoint regions and assigning each of `t` different threads to work on its own region. Assuming `n` is a multiple of `t`, each region has `n/t` elements. There are different ways that multiple threads might work on their assigned regions in parallel:

1.  **Using a shared global variable with a mutex:** Each thread sums into a shared global variable that is protected by a mutex. This approach can result in slow performance due to expensive synchronization operations (P and V) relative to the cost of a single memory update.
    
2.  **Using private variables:** Each peer thread computes its partial sum in a private variable that is not shared with any other thread. The main thread defines a global array, and each peer thread i accumulates its partial sum in the corresponding array element. Since each peer thread has a unique memory location to update, no mutexes are needed for protection. The main thread must wait for all children to finish and then sum up the elements of the array to arrive at the final result.
    
3.  **Using local variables:** Each peer thread accumulates its partial sum in a local variable, which reduces the number of memory references. This approach leads to even faster execution times compared to using private variables.
    

## What is an important lesson to learn from the parallel summing exercise?

Writing parallel programs is tricky. Seemingly small changes to the code have a significant impact on performance. Synchronization overhead is expensive and should be avoided if possible. If it cannot be avoided, the overhead should be amortized by as much useful computation as possible. Using local variables can help eliminate unnecessary memory references and improve performance.

## Why is the performance of the parallel program using a shared global variable with a mutex poor?

The performance of the parallel program using a shared global variable with a mutex is poor because the synchronization operations (P and V) are very expensive relative to the cost of a single memory update. This highlights an important lesson about parallel programming: Synchronization overhead is expensive and should be avoided if possible. If it cannot be avoided, the overhead should be amortized by as much useful computation as possible.

## How does the performance of the parallel program using private variables compare to the one using a shared global variable with a mutex?

The performance of the parallel program using private variables is orders of magnitude faster than the one using a shared global variable with a mutex. By giving each peer thread a unique memory location to update and not needing to protect these updates with mutexes, the program can run much faster.

## How does the performance of the parallel program using local variables compare to the ones using private variables and a shared global variable with a mutex?

The performance of the parallel program using local variables is even faster than the one using private variables, achieving another order-of-magnitude decrease in running time. By accumulating partial sums in local variables rather than global variables, the program reduces the number of memory references, further improving performance.

## What is the general technique used in many parallel applications to assign work to different threads?

The general technique used in many parallel applications to assign work to different threads is passing a small unique thread ID to the peer threads. Each peer thread uses its thread ID to determine which portion of the task it should work on.