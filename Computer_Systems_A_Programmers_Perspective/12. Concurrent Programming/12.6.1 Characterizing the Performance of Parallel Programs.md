## What is the ideal running time decrease for parallel programs?

In the ideal case, we would expect the running time of a parallel program to decrease linearly with the number of cores. That is, the running time should drop by half each time we double the number of threads. This is true until we reach a point where each core is busy running at least one thread. Beyond this point, <mark style="background: #FFB8EBA6;">running time might increase</mark> due to the overhead of context switching multiple threads on the same core. For this reason, parallel programs are often written so that each core runs exactly one thread.

<mark style="background: #FFB8EBA6;">pink</mark>
One such scenario is when a multi-threaded program is running on a system with a limited number of cores and the number of threads being run exceeds the number of cores. In such a situation, the operating system would be forced to switch between the threads frequently, leading to an increase in the running time of the program due to the overhead of context switching.

Another scenario could be when a multi-threaded program is running on a system with a large number of cores but the workload of the program is not well balanced across the threads. This can lead to some cores being heavily loaded while others are idle, which can also result in an increase in running time due to the overhead of context switching.

In both of these scenarios, it is important to ensure that the number of threads being run is optimized for the number of cores available on the system, and that the workload is well balanced across the threads, to minimize the overhead of context switching and maximize performance.
## What is the speedup of a parallel program and how is it defined?

The speedup of a parallel program is typically defined as: ```Sp = T1 / Tp```
where `p` is the number of processor cores and `Tk` is the running time on `k` cores. This formulation is sometimes referred to as strong scaling. When `T1` is the execution time of a sequential version of the program, then `Sp` is called the absolute speedup. When `T1` is the execution time of the parallel version of the program running on one core, then `Sp` is called the relative speedup.

Here's an example of measuring speedup in a parallel program:

Suppose we have a program that takes 10 seconds to run sequentially on a single core. When we run this program on 4 cores, it takes 5 seconds to complete.

The absolute speedup is: Sp = T1 / Tp = 10 / 5 = 2

This means that the program is running 2 times faster on 4 cores compared to a single core.

Now, let's say we have another version of the same program that is parallelized and it takes 7 seconds to run on a single core. When we run this version of the program on 4 cores, it takes 3 seconds to complete.

The relative speedup is: Sp = T1 / Tp = 7 / 3 = 2.33

This means that the program is running 2.33 times faster on 4 cores compared to a single core, but relative to the sequential version of the program, the absolute speedup is only 2 times.

## What is the difference between absolute speedup and relative speedup?

Absolute speedup is a truer measure of the benefits of parallelism than relative speedup. Parallel programs often suffer from synchronization overheads, even when they run on one processor, and these overheads can artificially inflate the relative speedup numbers because they increase the size of the numerator. On the other hand, absolute speedup is more difficult to measure than relative speedup because measuring absolute speedup requires two different versions of the program. For complex parallel codes, creating a separate sequential version might not be feasible, either because the code is too complex or because the source code is not available.

Example:
Let's consider a hypothetical parallel program that computes the sum of elements in an array. We have two versions of the program: a sequential version (`S`) and a parallel version (`P`). We will measure the absolute and relative speedup on a machine with 4 processor cores.

1.  **Sequential version (`S`):** The program takes 80 seconds to compute the sum.
2.  **Parallel version (`P`) with 1 core:** The program takes 60 seconds to compute the sum.
3.  **Parallel version (`P`) with 4 cores:** The program takes 20 seconds to compute the sum.

Now, let's calculate the absolute and relative speedup.

**Absolute speedup:**

In this case, we use the execution time of the sequential version (`S`) as `T1`.
Sp = T1 / Tp = 80s / 20s = 4

The absolute speedup is 4, which means that the parallel version running on 4 cores is 4 times faster than the sequential version.

**Relative speedup:**

In this case, we use the execution time of the parallel version (`P`) running on 1 core as `T1`.

`Sp = T1 / Tp = 60s / 20s = 3`

The relative speedup is 3, which means that the parallel version running on 4 cores is 3 times faster than the parallel version running on 1 core.

In this example, the absolute speedup provides a more accurate representation of the parallel program's performance improvement compared to the sequential program. The relative speedup may be misleading because it doesn't take into account the synchronization overheads in the parallel version running on a single core.

## What is efficiency in the context of parallel programming?

Efficiency, a related measure to speedup, is defined as: Ep = Sp / p = T1 / (p * Tp)
and is typically reported as a percentage in the range (0, 100]. Efficiency is a measure of the overhead due to parallelization. Programs with high efficiency are spending more time doing useful work and less time synchronizing and communicating than programs with low efficiency.

Example:
Let's consider the same hypothetical parallel program that computes the sum of elements in an array, with the same execution times as before:

1.  **Sequential version (`S`):** The program takes 80 seconds to compute the sum.
2.  **Parallel version (`P`) with 1 core:** The program takes 60 seconds to compute the sum.
3.  **Parallel version (`P`) with 4 cores:** The program takes 20 seconds to compute the sum.

Now, let's calculate the efficiency for the parallel version running on 4 cores.

`Efficiency (Ep) = Sp / p = T1 / (p * Tp)`

We already know the absolute speedup (Sp) is 4, and the number of processor cores (p) is 4.

`Ep = 4 / 4 = 1`

Now, we'll convert this value to a percentage.

`Ep (%) = 1 * 100 = 100%`

In this example, the parallel program has an efficiency of 100% when running on 4 cores. This means that the program is spending more time doing useful work and less time on synchronization and communication overhead. A high efficiency value like this indicates that the program is effectively utilizing the available parallelism. However, keep in mind that this is a simplified example, and in practice, achieving such high efficiency can be challenging due to the complexity of real-world problems and the overheads associated with parallelization.

However, if we had a parallel program with a relative speedup of 2 but running on 8 cores, the efficiency would be:

`Ep = Sp / p = 2 / 8 = 0.25`

The efficiency is 0.25, which means that only 25% of the available processing power is being used effectively and efficiently. The remaining 75% is being wasted due to parallelization overhead.

## What is the difference between weak scaling and strong scaling?

There is another view of speedup, known as weak scaling, which increases the problem size along with the number of processors, such that the amount of work performed on each processor is held constant as the number of processors increases. With this formulation, speedup and efficiency are expressed in terms of the total amount of work accomplished per unit time.

Weak scaling is often a truer measure than strong scaling because it more accurately reflects our desire to use bigger machines to do more work. This is particularly true for scientific codes, where the problem size can be easily increased, and where bigger problem sizes translate directly to better predictions of nature. However, there exist applications whose sizes are not so easily increased, and for these applications, strong scaling is more appropriate.

Example:
Let's consider a hypothetical parallel program that computes the sum of elements in an array. The problem size is defined as the number of elements in the array.

1.  **Sequential version (`S`):** The program takes 80 seconds to compute the sum of 1000 elements in the array.
    
2.  **Parallel version (`P`) with 1 core:** The program takes 60 seconds to compute the sum of 1000 elements in the array.
    
3.  **Parallel version (`P`) with 4 cores:** The program takes 20 seconds to compute the sum of 4000 elements in the array.
    

Now, let's calculate the weak scaling efficiency.

**Weak scaling efficiency:**

Efficiency is defined as the ratio of the speedup to the number of cores used.

`Ep = Sp / p = T1 / (p * Tp)`

In this case, let's use the execution time of the sequential version (`S`) as `T1`.

For the parallel version with 1 core, `Ep = 80s / (1 * 60s) = 1.33`

For the parallel version with 4 cores, `Ep = 80s / (4 * 20s) = 1`

The weak scaling efficiency of the parallel program decreases as the number of cores increases. In other words, the overhead due to parallelization increases as the number of cores increases. This is because the problem size also increases with the number of cores, which results in increased overhead from communication and synchronization between the cores.

In this example, the weak scaling efficiency provides a more accurate representation of the parallel program's performance improvement compared to the sequential program as the problem size increases.