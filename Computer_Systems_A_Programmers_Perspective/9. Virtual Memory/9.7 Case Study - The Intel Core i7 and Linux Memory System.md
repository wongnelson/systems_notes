The processor package (chip) includes four cores, a large L3 cache shared by all of the cores, and a DDR3 memory controller. Each core contains<mark style="background: #FFB8EBA6;"> a hierarchy of TLBs</mark>, <span style="background:#fff88f">a hierarchy of data and instruction caches</span>, and a set of fast point-to-point links, based on the QuickPath technology, for communicating directly with the other cores and the external I/O bridge. <span style="background:#d3f8b6">The L1, L2, and L3 caches are physically addressed</span>, with a block size of 64 bytes. L1 and L2 are 8-way set associative, and L3 is 16-way set associative. The page size can be configured at start-up time as either 4 KB or 4 MB. Linux uses 4 KB pages.

<mark style="background: #FFB8EBA6;">pink:</mark>
The TLBs at the higher levels of the hierarchy, such as the L1 and L2 TLBs, are smaller and faster but have less capacity, while the TLBs at the lower levels, such as the L3 TLB, are larger and slower but have more capacity. This is because the higher-level TLBs are closer to the processor cores and can access the data more quickly, but they have limited capacity and cannot store all of the translations.

When a processor core needs to access a memory location, it first checks the TLBs at the highest level of the hierarchy (such as the L1 TLB). If the translation is not found in the TLB, the core moves down the hierarchy to the next level (such as the L2 TLB), and so on, until it finds the translation it needs. This process is called TLB miss handling, and it can take several clock cycles to complete, which can slow down the performance of the processor.

Having a hierarchy of TLBs allows for efficient memory access by reducing the amount of time required to search for a translation while still allowing for a large amount of virtual memory to be used. It also helps to balance the trade-off between the speed and size of the TLBs by providing multiple levels of cache with different capacities and access times.

<span style="background:#fff88f">yellow</span>:

In a computer system, a "hierarchy of data" typically refers to the organization of different levels of memory hierarchy.

At the top of the hierarchy is the fastest, but smallest, memory cache, typically referred to as the L1 cache. The L1 cache is located within the processor core itself, and it is used to store frequently accessed data and instructions that are needed by the core.

The next level in the hierarchy is the L2 cache, which is larger than the L1 cache but slower. The L2 cache is usually shared by multiple processor cores, and it serves as a buffer between the L1 cache and the main memory.

The third level in the hierarchy is the L3 cache, which is even larger than the L2 cache but slower still. The L3 cache is typically shared by all the cores on the processor and serves as a global cache for frequently accessed data.

Beyond the L3 cache, there is typically a main memory, which is much larger than the caches but also much slower. The main memory is used to store data and instructions that are not currently being used by the processor, as well as data that is too large to fit in the caches.

Finally, at the bottom of the hierarchy is secondary storage, such as a hard disk drive or solid-state drive, which is used for long-term storage of data and instructions that are not currently being used.

The organization of data in a hierarchy allows for faster access to frequently used data, while still allowing for a large amount of memory to be available. By storing data at different levels of the hierarchy, computer systems can balance the need for speed with the need for storage capacity, resulting in efficient use of resources.

<span style="background:#d3f8b6">green</span>:
L1, L2, and L3 caches are physically addressed because they are part of the processor's memory hierarchy and operate at the physical address level.

When a processor accesses memory, it does so by sending a physical memory address to the memory subsystem. The physical memory address is used to identify the location of the data in memory and is translated by the memory management unit (MMU) into a virtual memory address, which is used by the operating system and applications.

The L1, L2, and L3 caches are located between the processor core and the main memory, and they operate at the physical address level. This means that they store data and instructions using physical memory addresses, which are the actual addresses of the data in memory. When the processor requests data or instructions from memory, the caches check to see if the data is already stored in the cache using its physical address.

Storing data in this way allows the caches to operate very quickly, as they can respond to memory requests with minimal latency. Additionally, it helps to reduce the amount of traffic on the memory bus, as the caches can satisfy many memory requests without needing to access the main memory.

In contrast, virtual memory addresses are used by the operating system and applications to refer to data and instructions in memory. The virtual addresses are translated into physical addresses by the MMU, which maps them to the actual locations in memory. However, the caches do not use virtual memory addresses, as they operate at the physical address level to provide fast and efficient memory access.