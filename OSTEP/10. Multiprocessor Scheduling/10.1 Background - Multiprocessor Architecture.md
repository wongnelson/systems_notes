## How does a cache work in a single-CPU system?

Caches operate on the concept of locality. There are two types of locality: temporal locality and spatial locality. Temporal locality suggests that when a piece of data is accessed, it is likely to be accessed again in the near future. Spatial locality implies that if a program accesses a data item at address x, it's probable that data items near x will also be accessed. 

Consider an example where a program issues a load instruction to fetch a value from memory. Initially, this data resides in the main memory and takes a while to fetch. The processor, assuming the data may be reused, places a copy of this data into the cache. If the program fetches the same data item again, the CPU first checks the cache. If the data is present, it can be fetched much more quickly, thus enhancing the program's speed.

## What happens to caching when there are multiple CPUs in a system?

Caching with multiple CPUs is considerably more complex. For instance, imagine a program running on CPU 1 reads a data item (value D) at address A. This data is fetched from the main memory since it's not in CPU 1's cache, and D is fetched. The program then modifies the value at address A, updating its cache with the new value D'. Subsequently, the OS moves the program to CPU 2. The program then re-reads the value at address A. However, as there's no data in CPU 2's cache, the system fetches the value from main memory, getting the old value D instead of the correct value D'. This situation is known as the cache coherence problem.

### How is the cache coherence problem solved?

The cache coherence problem is primarily addressed by hardware mechanisms. One commonly used technique is bus snooping in bus-based systems. Here's how it works:

1. **Bus Snooping**: In a bus-based system, each cache monitors the bus that connects them to the main memory. Whenever a memory update occurs, caches "snoop" on the bus to observe these updates.

2. **Detecting Updates**: When a cache sees an update for a data item it holds in its cache, it detects the change by monitoring the bus activity. The cache recognizes that the data it holds may no longer be valid or consistent.

3. **Maintaining Coherence**: To maintain cache coherence, the cache takes appropriate actions based on the observed update. It can either invalidate its copy of the data (removing it from the cache) or update it with the new value (placing the updated value into its cache).

4. **Write-Back Caches**: In the case of write-back caches, where modifications are not immediately written back to the main memory, additional complexity arises. However, the basic scheme of bus snooping still applies. The write to the main memory becomes visible to other caches at a later time when the modified data is eventually written back.

To illustrate this process, consider the following example:

```ascii
CPU 1 cache: A -> D'
Main memory: A -> D
CPU 2 cache: No data
```

In this example, CPU 1 holds a cached copy of data A with an updated value D'. CPU 2 doesn't have any data from that memory location in its cache.

After bus snooping, the state changes as follows:

```ascii
CPU 1 cache: A -> D'
Main memory: A -> D'
CPU 2 cache: No data
```

Now, when CPU 2 attempts to read the value at address A, it will receive the updated value D' from the main memory. This ensures that a single shared memory view is maintained among the different caches in the system, resolving the cache coherence problem.