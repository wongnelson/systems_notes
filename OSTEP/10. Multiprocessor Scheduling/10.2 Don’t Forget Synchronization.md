## Why is synchronization necessary when accessing shared data?

Despite caches working to provide coherence, programs, or the operating system itself, still need to address synchronization when accessing shared data. When updating shared data items or structures across CPUs, mutual exclusion primitives, such as locks, should likely be used to guarantee correctness. This approach is necessary to ensure that concurrent operations on shared data structures occur correctly. 

For example, consider a shared queue being accessed concurrently on multiple CPUs. Without locks, adding or removing elements from the queue concurrently could lead to unexpected outcomes, even with underlying coherence protocols in place. Locks are needed to atomically update the data structure to its new state.

## Can you provide an example to demonstrate the need for synchronization?

Imagine we have the following code sequence to remove an element from a shared linked list:

```C
typedef struct __Node_t {
    int value;
    struct __Node_t *next;
} Node_t;

int List_Pop() {
    Node_t *tmp = head;   // remember old head 
    int value = head->value; // ... and its value
    head = head->next; // advance head to next pointer
    free(tmp); // free old head

    return value;
}
```

If two threads on different CPUs enter this routine simultaneously, each thread will try to remove the same head element instead of each thread removing an individual element from the list's head. This scenario can lead to various problems, such as an attempted double free of the head element and potentially returning the same data value twice.

## How can such concurrency problems be solved?

Concurrency problems like the one described above can be solved using locking. In this case, allocating a simple mutex (e.g., `pthread_mutex_t m;`) and then adding a lock(`&m`) at the beginning of the routine and an unlock(`&m`) at the end will solve the problem, ensuring the code executes as desired. Here's how the code might look:

```C
pthread_mutex_t m;

int List_Pop() {
    pthread_mutex_lock(&m); // add lock

    Node_t *tmp = head; // remember old head 
    int value = head->value; // ... and its value
    head = head->next; // advance head to next pointer
    free(tmp); // free old head

    pthread_mutex_unlock(&m); // add unlock

    return value;
}
```

This ensures that only one thread can execute the `List_Pop` function at a time, preventing the double free and double return issues described earlier.

## What are the drawbacks of synchronization?

While synchronization through locking ensures correctness, it comes with some issues, especially regarding performance. As the number of CPUs grows, access to a synchronized shared data structure becomes quite slow. This is due to the fact that each CPU must wait its turn to access the shared resource, leading to increased waiting times and potentially reducing the advantages of concurrency.