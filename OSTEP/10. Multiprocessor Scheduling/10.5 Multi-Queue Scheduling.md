## What is Multi-Queue Multiprocessor Scheduling (MQMS)?

Multi-Queue Multiprocessor Scheduling (MQMS) is an approach to building a scheduler for a multiprocessor system that utilizes multiple queues, typically one per CPU. In this framework, each scheduling queue follows a specific scheduling discipline, such as round robin, but any algorithm can be used.

When a job enters the system, it's placed into one scheduling queue based on some heuristic (for instance, randomly or into a queue with fewer jobs than the others). Then it's scheduled essentially independently, which avoids the problems of information sharing and synchronization that occur in the single-queue approach.

For instance, consider a system with two CPUs (CPU 0 and CPU 1) and four jobs (A, B, C, and D). The operating system might distribute the jobs between the queues like this:

```
Q0: A -> C -> NULL
Q1: B -> D -> NULL
```

<mark style="background: #FFB8EBA6;">Each CPU now has two jobs to choose from,</mark> and the round-robin system might produce a schedule like this:

```
CPU 0: A -> A -> C -> C -> A -> A -> C -> C ...
CPU 1: B -> B -> D -> D -> B -> B -> D -> D ...
```

<mark style="background: #FFB8EBA6;">pink</mark>:
The statement "Each CPU now has two jobs to choose from" refers to the initial state of the system. In your example, CPU 0 has jobs A and C in its queue (Q0), and CPU 1 has jobs B and D in its queue (Q1). In other words, each CPU has two jobs in its own queue that it can potentially execute.

The round-robin scheduling algorithm works by giving each job in the queue a chance to execute for a fixed amount of time (also known as a time slice or time quantum). When a job's time quantum expires, it's moved to the back of the queue, and the next job in the queue is given a chance to execute. This process continues until all jobs have been completed.

In your example, each CPU alternates between the two jobs in its queue. CPU 0 executes job A for two time quanta, then executes job C for two time quanta, then goes back to job A, and so on. CPU 1 does the same with jobs B and D.

Note that the way this example is presented is slightly simplified. In a real system, jobs can block waiting for I/O, they can create new jobs (by calling fork, for example), and jobs can finish (when they've done all the work they're going to do). This means that the contents of the queue can change over time, and it's not always as simple as just rotating between a fixed set of jobs.

## What are the advantages of MQMS over SQMS?

MQMS has a distinct advantage over SQMS in that it's more scalable. As the number of CPUs grows, so too does the number of queues, which should help prevent lock and cache contention from becoming central problems.

Moreover, MQMS intrinsically provides cache affinity; jobs stay on the same CPU, allowing them to benefit from reusing cached contents.

## What is the main challenge in the multi-queue-based approach?

The main challenge in the multi-queue-based approach is load imbalance. For instance, if we take the previous example and one of the jobs (say, C) finishes, we might have these scheduling queues:

```
Q0: A -> NULL
Q1: B -> D -> NULL
```

When we run our round-robin policy on each queue, job A gets twice as much CPU time as jobs B and D, which is not the desired outcome. An even worse situation arises if both A and C finish, leaving just jobs B and D in the system:

```
Q0: NULL
Q1: B -> D -> NULL
```

In this case, CPU 0 remains idle, resulting in inefficient CPU usage.

## How can a multi-queue multiprocessor scheduler handle load imbalance?

The key technique to addressing load imbalance is job migration—moving a job from one CPU to another. This migration process can balance the load across all CPUs.

For instance, in the situation where one CPU is idle, and the other has some jobs:

```
Q0: NULL
Q1: B -> D -> NULL
```

The operating system should simply move one of B or D to CPU 0, resulting in balanced load across the CPUs.

In more complicated situations, continuous migration of one or more jobs may be required. Consider the situation where A was alone on CPU 0 and B and D were alternating on CPU 1:

```
Q0: A -> NULL
Q1: B -> D -> NULL
```

One possible solution is to keep switching jobs:

```
CPU 0: A -> A -> A -> B -> A -> B -> A -> B -> B -> B -> B ...
CPU 1: B -> D -> B -> D -> D -> D -> D -> D -> A -> D -> A -> D ...
```

While several other migration patterns are possible, deciding when to enact such a migration can be a complex task.

## How can the system decide to enact job migration?

One basic approach is to use a technique known as work stealing. In this approach, a source queue that is low on jobs occasionally peeks at a target queue. If the target queue is more full than the source queue, the source steals one or more jobs from the target to help balance the load.

However, there's a trade-off involved. If you look at other queues too often, you suffer from high overhead, which hampers scalability—the very reason you implemented multiple queue scheduling in the first place. If you don't look at other queues often enough, you risk severe load imbalances. Hence, finding the right threshold remains a challenging aspect of system policy design.