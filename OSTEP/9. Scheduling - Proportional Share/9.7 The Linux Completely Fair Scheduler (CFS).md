## What is the Linux Completely Fair Scheduler (CFS)?

The Linux Completely Fair Scheduler (CFS) is the current Linux approach to fair-share scheduling. It implements fair-share scheduling in an efficient and scalable manner. Unlike traditional schedulers which are based around fixed time slices, the CFS operates through a simple counting-based technique known as virtual runtime (vruntime).

The main goal of CFS is to reduce overhead in CPU usage during scheduling. As revealed in studies conducted on Google datacenters, scheduling uses about 5% of overall datacenter CPU time even after aggressive optimization. Therefore, the CFS aims to reduce this overhead as much as possible.

```ascii
    A B C C C A C C ...104
    SCHEDULING: PROPORTIONAL SHARE
    A B C D A B C D A B A B A B
    0
    50
    100
    Time
```

*Figure 9.4: CFS Simple Example*

## How does the Linux Completely Fair Scheduler (CFS) work?

The working of the CFS revolves around the concept of vruntime. As each process runs, it accumulates vruntime. In most cases, each process's vruntime increases at the same rate, in proportion with physical (real) time. The process with the lowest vruntime is picked to run next when a scheduling decision occurs. 

## What is the role of `sched_latency` in the Linux Completely Fair Scheduler (CFS)?

In the Linux CFS, `sched_latency` is a control parameter that plays a key role in balancing fairness and performance. Essentially, it represents the time window during which CFS aims to distribute the CPU time among all running processes fairly.

## How does `sched_latency` determine the time slice of each process?

The time slice of each process is not fixed in CFS. Instead, it is dynamically determined based on the value of `sched_latency` and the number of currently running processes. 

In an ideal scenario where `sched_latency` is set to 48 milliseconds and there are `n` running processes, the time slice for each process would be `sched_latency / n`. 

For instance, if there are `n = 4` processes running, CFS would ideally divide the `sched_latency` by `n` to arrive at a per-process time slice of 12 ms. This means that each of the four jobs would be expected to run for roughly 12 ms within the 48 ms `sched_latency` period.

However, the actual time a job runs can vary due to a number of factors, including the system's current load, the job's priority, and other variables. This is why the time slice is a target rather than a strict limit. 

When a process has run for its time slice duration, CFS checks if there's another process with a lower `vruntime` (virtual runtime) that should be given a chance to run. If such a process exists, a context switch occurs, and that process is given the CPU.

## What is the role of the min granularity parameter in the CFS?

In cases where there are too many processes running, this could potentially lead to too small of a time slice and thus too many context switches. To address this issue, CFS introduces another parameter: min granularity. This is usually set to a value like 6 ms. CFS will never set the time slice of a process to less than this value, ensuring that not too much time is spent in scheduling overhead.

For example, if there are ten processes running, our original calculation would divide sched latency by ten to determine the time slice (result: 4.8 ms). However, because of min granularity, CFS will set the time slice of each process to 6 ms instead. Although CFS wonâ€™t be perfectly fair over the target scheduling latency (sched latency) of 48 ms, it will be close, while still achieving high CPU efficiency.

## How does the CFS handle timer interrupts?

CFS utilizes a periodic timer interrupt, which means it can only make decisions at fixed time intervals. This interrupt goes off frequently (e.g., every 1 ms), giving CFS a chance to wake up and determine if the current job has reached the end of its run. If a job has a time slice that is not a perfect multiple of the timer interrupt interval, that is OK; CFS tracks vruntime precisely, which means that over the long haul, it will eventually approximate ideal sharing of the CPU.